{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "div.prompt {display:none}\n",
    "</style>\n",
    "\n",
    "# Procesamiento del habla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modelo del habla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Para empezar a hablar de procesamiento del habla tenemos que tener un modelo de cómo se produce el habla. El modelo que usamos es este: tenemos una excitación que es la entrada a un sistema, esta excitación decimos que es un tres de deltas si es fonema voiced, o es ruido si es unvoiced. El filtro $H(z)$ es lo que distingue entre fonemas. Por ejemplo, si es una 'a' el fonema, la entrada es el tren de deltas y el $H(z)$ es el filtro correspondiente a la 'a'. Si es una 'f' la entrada es la de abajo y el $H(z)$ es el filtro de la 'f'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Excitación $x(n)$ $\\rightarrow$ filtro $H(z)$ $\\rightarrow$ señal de audio $s(n)$\n",
    "    * Voiced: tren de deltas\n",
    "    * Unvoiced: ruido\n",
    "* $H(z)$ distinto para cada fonema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T00:49:21.479923Z",
     "start_time": "2018-08-06T00:49:21.352719Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"speech_filter_model.png\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Lo importante de esto es que lo que nos da la información acerca de qué fonema se dice es básicamente el filtro $H$.\n",
    "\n",
    "Entonces si tenemos una señal de audio $s[n]$ en la que queremos hacer reconocimiento, tenemos que de alguna forma obtener $H(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cepstrum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T16:05:01.098895Z",
     "start_time": "2018-08-06T16:05:01.083442Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Según lo del grafico anterior, la señal de audio $s[n]$ es la excitación $x[n]$ pasada por el filtro $h[n]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Obtener $H(z)$ a partir de $s(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$s(n) = x(n)*h(n)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-06T16:06:34.679656Z",
     "start_time": "2018-08-06T16:06:34.664752Z"
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Para hacer la desconvolución, separar $x(n)$ de $h(n)$ obviamente tenemos que trabajar con el espectro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$S(\\omega)=X(\\omega).H(\\omega)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    " En frecuencia tenemos un producto, pero esto sigue siendo difícil de separar. Entonces lo que hacemos es aplicar el logaritmo al espectro, y esto resulta en una suma, que es mucho más fácil de separar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\log|S(\\omega)|=\\log|X(\\omega)|+\\log|H(\\omega)|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T12:28:52.492147Z",
     "start_time": "2018-08-08T12:28:52.365325Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\\log |S(\\omega)|$$\n",
    "\n",
    "<img src=\"log_spectrum_salida.png\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "$$\\log |H(\\omega)|$$\n",
    "\n",
    "<img src=\"log_spectrum_h.png\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "$$\\log |X(\\omega)|$$\n",
    "\n",
    "<img src=\"log_spectrum_x.png\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Vistos como señales:\n",
    "\n",
    "$\\log |H(\\omega)|$: frecuencias bajas\n",
    "\n",
    "$\\log |X(\\omega)|$: frecuencias altas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\log|S(\\omega)|=\\log|X(\\omega)|+\\log|H(\\omega)|$ $\\Rightarrow$ IDFT $\\Rightarrow$ $\\hat{s}(n)=\\hat{x}(n) + \\hat{h}(n)$\n",
    "\n",
    "__Liftering:__\n",
    "* $\\hat{h}(n):$ primeros $N$ coeficientes\n",
    "* $\\hat{x}(n):$ últimos coeficientes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$s(n) \\rightarrow DFT \\Rightarrow \\log \\Rightarrow IDFT \\rightarrow \\hat{s} (n)$$\n",
    "\n",
    "$$\\hat{s}(n) \\rightarrow Lifter \\rightarrow\\hat{h} (n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Ahora que ya tenemos esta suma, si aplicamos otra vez una transformada o antitransformada, (según la versión), vemos que la parte correspondiente a $H$ queda en la parte más baja, mientras que la de $X$ queda en los coeficientes más altos. Entonces podemos simplemente tomar los primeros N coeficientes de $\\hat{s}$ y nos quedamos con los coeficientes de $\\hat{h}$. Con esto entonces ya tenemos la información del filtro H. Si bien podemos convertir este $\\hat{H}$ a $H(\\omega)$ o incluso a $h[n]$, en realidad no hace falta para el reconocimiento del habla y podemos usar los coeficientes Cepstrum directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## MFCC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* La sensibilidad del oído no es la misma para los distintos rangos de frecuencias\n",
    "* Un valor \"promediado\" para cada rango:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"mel.png\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$F(m)=\\log \\big\\{ \\sum_{k=0}^{N-1} |S(k)|^2 H_m(k) \\big\\} \\ , \\ 0 \\leq m \\leq M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$\\hat{s}(n)=\\sum_{m=0}^{M-1} F(m) \\cos\\big(\\pi n(m+\\frac{1}{2})/M\\big) \\ , \\ 0 \\leq n \\leq M$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$s(n) \\rightarrow DFT \\Rightarrow Mel \\Rightarrow \\log \\Rightarrow DCT \\rightarrow \\hat{s} (n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T16:12:51.269304Z",
     "start_time": "2018-08-05T16:12:51.255743Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Etiquetar a los MFCC con el fonema correspondiente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aprendizaje supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T16:16:56.390431Z",
     "start_time": "2018-08-05T16:16:56.378471Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Set de entrenamiento: $x_1,...,x_N$ __y__ las etiquetas $z_1,...,z_N \\in 1,...,K$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1) Maximum likelihood: \n",
    "\n",
    "$$\\mu_{ML}^k= \\frac{\\sum_{z_i=k} x_i}{\\#(z_i = k)}$$\n",
    "\n",
    "$$\\mu_{ML} = \\frac{1}{N} \\sum_{i=1}^N x_i$$\n",
    "\n",
    "$$\\Sigma_{ML}=\\frac{1}{N} \\sum_{i=1}^N(x_i-\\mu_{ML})(x_i-\\mu_{ML})^T$$\n",
    "\n",
    "* Igual $\\Sigma$ para todo $k$ $\\Rightarrow$ separación lineal entre clases (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-07T20:17:53.986577Z",
     "start_time": "2018-08-07T20:17:53.964762Z"
    }
   },
   "source": [
    "2) Obtener $k$ que maximice $P(k|x) = \\frac{P(x|k)P(k)}{P(x)} \\Rightarrow P(x|k)P(k)$\n",
    "\n",
    "$$P(x|k)=\\mathcal{N}(x;\\mu_{ML}^k,\\Sigma_{ML})$$\n",
    "\n",
    "$$P(k)=\\frac{\\# (z_i = k)}{N}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Aprendizaje NO supervisado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T16:16:56.390431Z",
     "start_time": "2018-08-05T16:16:56.378471Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Set de entrenamiento: $x_1,...,x_N$ __sin__ las etiquetas $z_1,...,z_N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algoritmo EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Asigna una probabilidad a cada $x$ de pertenecer a cada clase $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T16:24:25.496342Z",
     "start_time": "2018-08-05T16:24:25.484134Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1) __Inicializar__ $\\mu_k$, $\\sigma_k^2$, y $\\pi_k=P(k)$ para cada clase (p.ej.: K-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T16:27:33.157516Z",
     "start_time": "2018-08-05T16:27:33.143038Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2) __Paso E__: Calcular $\\gamma_k(x) = P(z=k|x)$: probabilidad de que la muestra $x$ pertenezca a la clase $k$.\n",
    "\n",
    "$$\\gamma_k(x) = P(z=k|x)=\\frac{P(x|z=k)P(z=k)}{P(x)}$$\n",
    "\n",
    "$$\\gamma_k(x) = \\frac{\\pi_k \\ \\mathcal{N}(x;\\mu_k,\\Sigma_k)}{\\sum_{l=1}^K \\pi_l \\ \\mathcal{N} (x;\\mu_l,\\Sigma_l)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3) __Paso M__: Reestimar $\\mu_k$, $\\sigma_k^2$, y $\\pi_k=P(k)$ para cada clase.\n",
    "\n",
    "$$\\mu_k=\\frac{\\sum_{i=1}^N \\gamma_k(x_i) x_i}{\\sum_{i=1}^N \\gamma_k(x_i)}$$\n",
    "\n",
    "$$\\Sigma_k=\\frac{\\sum_{i=1}^N \\gamma_k(x_i) (x_i-\\mu_k)(x_i-\\mu_k)^T}{\\sum_{i=1}^N \\gamma_k(x_i)}$$\n",
    "\n",
    "$$\\pi_k=\\frac{1}{N}\\sum_{i=1}^N \\gamma_k(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4) Calcular __log-likelihood__\n",
    "\n",
    "$$P(x_i) = \\sum_{k=1}^K \\pi_k \\mathcal{N} (x_i|\\mu_k,\\sigma_k^2)$$\n",
    "\n",
    "$$LL = \\log \\prod_{i=1}^N P(x_i) = \\sum_{i=1}^N \\log P(x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5) Repetir 2, 3, 4 hasta que el log-likelihood no cambie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cadenas de Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Estados: $1,...,N$\n",
    "- Transiciones: probabilidades $a_{ij}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Experimento: generar secuencia de estados $Q=q_1,...,q_T$\n",
    "\n",
    "* $P(qt|q_{t-1},q_{t-2},...q_1) = P(q_t|q_{t-1})$ : la probabilidad de pasar de un estado a otro NO DEPENDE de todo la secuencia de estados previa\n",
    "\n",
    "* $a_{ij} = P(q_t=j|q_{t-1}=i)$: la probabilidad de pasar de un estado a otro es constante en el tiempo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cadenas de Markov Ocultas (HMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- En cada estado se genera una observación $\\Rightarrow$ distribución de probabilidad\n",
    "- $Q = q_1,q_2,...q_T$ : secuencia de estados\n",
    "- $Y = y_1,y_2,...,y_T$ : secuencia de observaciones\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T17:08:14.205125Z",
     "start_time": "2018-08-05T17:08:14.186550Z"
    },
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* $P(y_t|Q,Y_{-t}) = P(y_t|q_t=j)$: la observación generada en un estado sólo depende de cuál es el estado actual $\\Rightarrow P(y_t|q_t=j)=b_j(y_t)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Modelo: $\\lambda=\\{\\{i\\},\\{a_{ij}\\},\\{b_j(y_t)\\}\\}$, con $i,j = 1,...,N$, $t = 1,...,T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T13:18:21.854484Z",
     "start_time": "2018-08-08T13:18:21.843618Z"
    }
   },
   "source": [
    "\n",
    "Para habla: ${a_{ij}}$: \"hacia adelante\" \n",
    "\n",
    "<img src=\"markov_simple.svg\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algoritmo de Viterbi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Dado $\\lambda$ y una secuencia de observaciones $Y=y_1,...,y_T$, hallar la secuencia de estados óptima $Q^*=q_1^*,...,q_T^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "$\\DeclareMathOperator*{\\argmax}{argmax}$\n",
    "$Q^*=\\argmax\\limits_{\\forall \\ Q} P(Q|Y)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\phi_t(j)=\\max\\limits_{\\forall \\ Q_{t-1}} P(Q_{t-1},q_t=j,Y_t)$: probabilidad del camino óptimo hasta $t$ que genera la secuencia de observaciones $Y_t=y_1,...,y_t$, terminando en el estado $j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1) __Inicialización__:\n",
    "\n",
    "$\\pi_i$: probabilidad de empezar en el estado $i$\n",
    "\n",
    "$\\phi_1(i)=b_i(y_1)\\pi_i$\n",
    "\n",
    "$\\psi_1(i)=0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2) __Recursión__:\n",
    "\n",
    "$\\phi_t(j)=b_j(y_t)\\max\\limits_{1\\leq i \\leq N} a_{ij} \\phi_{t-1}(i)$\n",
    "\n",
    "$\\psi_t(j)=\\argmax\\limits_{1 \\leq i \\leq N}  a_{ij} \\phi_{t-1}(i)$\n",
    "\n",
    "Camino óptimo $S^* = s_1^*,...,s_t^*$ con $s_t^*=j$ $\\Rightarrow$ $\\psi_t(j)$ nos da $s_{t-1}^*$, el estado anterior a $j$.\n",
    "\n",
    "$\\phi_t(j)$: probabilidad de ese camino."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3) __Terminación y backtracking__:\n",
    "\n",
    "Primero el último estado:\n",
    "\n",
    "\n",
    "$q_T^*=\\argmax\\limits_{1 \\leq j \\leq N} \\phi_T(j)$\n",
    "\n",
    "Después:\n",
    "\n",
    "\n",
    "\n",
    "$q_t^*=\\psi_{t+1}(q_{t+1}^*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Entrenamiento de la HMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T18:46:22.273662Z",
     "start_time": "2018-08-05T18:46:22.261985Z"
    },
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Dado $Y$, estimar $\\lambda=\\{\\{i\\},\\{a_{ij}\\},\\{b_j(y_t)\\}\\}$, con $i,j = 1,...,N$, $t = 1,...,T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Algoritmo Baum-Welch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "* Basado en EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1) __Inicialización__ de $\\mu_k$, $\\Sigma_k$ y $\\{a_{ij}\\}$.\n",
    "\n",
    "\\begin{align*} \n",
    "\\{a_{ij}\\} =\\   &0.5 \\ \\text{quedarse en }i\\\\ \n",
    "&0.5 \\ \\text{ir a } j\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2) __Paso E__:\n",
    "\n",
    "__Recursión backward__\n",
    "\n",
    "$\\alpha_t(j)=P(y_1,...,y_t,q_t=j)$: probabilidad de la secuencia de observaciones hasta $t$ y que el estado actual sea $j$\n",
    "\n",
    "$\\alpha_t(j)=\\sum_{i=1}^N b_j(y_t)a_{ij}\\alpha_{t-1}(i)$\n",
    "\n",
    "__Recursión forward__\n",
    "\n",
    "$\\beta_t(i)=P(y_t+1,...,y_T|q_t=i)$: probabilidad de que se dé la secuencia de observaciones desde $t+1$ hasta el final, dado que el estado actual es $i$\n",
    "\n",
    "$\\beta_t(i)=\\sum_{j=1}^Nb_j(y_{t+1})a_{ij}\\beta_{t+1}(j)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\gamma_t(i)=P(q_t=i|Y)$: probabilidad de que el estado en el instante $t$ sea $i$, dada la secuencia de observaciones\n",
    "\n",
    "$\\gamma_t(i)=\\frac{\\alpha_t(i)\\beta_t(i)}{\\sum_{j=1}^N\\alpha_t(j)\\beta_t(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$\\xi_t(i,j)=P(q_t=i,q_{t+1}=j|Y)$: probabilidad de estar en el estado $i$ en el instante $t$ y pasar al estado $j$, dada la secuencia de observaciones\n",
    "\n",
    "$\\xi_t(i,j)=\\frac{\\alpha_t(i)\\beta_{t+1}(j)b_j(y_{t+1})a_{ij}}{\\sum_{j=1}^N \\alpha_t(j)\\beta_t(j)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3) __Paso M__: reestimar los parámetros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\mu_j=\\frac{\\sum_{t=1}^T \\gamma_t(j)y_t}{\\sum_{t=1}^T\\gamma_t(j)}$: media para el estado $j$: promedio ponderado de las observaciones con las probabilidades de que cada observación se dé en el estado $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$\\Sigma_j=\\frac{\\sum_{t=1}^T (y_t-\\mu_j)^T(y_t-\\mu_j)\\gamma_t(j)}{\\sum_{t=1}^T\\gamma_t(j)}$: varianza para el estado $j$: varianza ponderada con las probabilidades de que cada observación se dé en el estado $j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$a_{ij}=\\frac{\\sum_{t=1}^T \\xi_t(i,j)}{\\sum_{t=1}^T\\gamma_t(i)}$: probabilidad de transición del estado $i$ al $j$: suma de las probabilidades de pasar del estado $i$ al estado $j$ en todos los instantes, dividido por la probabilidad total de transición desde el estado $i$ hacia otro estado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4) Calcular __log-likelihood__\n",
    "\n",
    "$$P(X) = \\sum_{i=1}^N \\alpha_{t_0}(i) \\beta_{t_0}(i)$$\n",
    "\n",
    "$$LL = \\log P(X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5) Repetir 2, 3, 4 hasta que el log-likelihood no cambie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T19:24:45.664857Z",
     "start_time": "2018-08-05T19:24:45.655082Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Aplicación al reconocimiento de habla"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-05T19:25:13.239342Z",
     "start_time": "2018-08-05T19:25:13.227963Z"
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "1) Set de entrenamiento: frases grabadas y sus transcripciones fonéticas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "2) Parametrización: coeficientes MFCC por cada ventana "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "3) HMM:\n",
    "\n",
    "* 3 estados emisores por fonema + 2 estados no emisores (inicial y final) para unirlos:\n",
    "<img src=\"markov_fonema.svg\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4) Entrenamiento:\n",
    "\n",
    "* Por cada frase:\n",
    "\n",
    "    \"la casa es linda\":\n",
    "\n",
    "<img src=\"markov_frase.svg\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "\n",
    "* No hace falta segmentar la frase en fonemas \"a mano\"\n",
    "\n",
    "* L frases:\n",
    "    $$\\mu_j=\\frac{\\sum_{l=1}^L\\sum_{t=1}^{T_l}\\gamma_t^l(j)y_t^l}{\\sum_{l=1}^L\\sum_{t=1}^{T_l}\\gamma_t^l(j)}$$\n",
    "    ... lo mismo para $\\Sigma_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "4) Modelo de lenguaje:\n",
    "\n",
    "<img src=\"modelo_lenguaje.svg\" style=\"display:block; margin-left: auto; margin-right: auto;\"/>\n",
    "\n",
    "Para las transiciones entre palabras: bigramas:\n",
    "\n",
    "$$P(w_4|w_3,w_2,w_1) \\simeq P(w_4|w_3)$$\n",
    "\n",
    "$$P(w_4|w_3)=\\frac{N(w_3 w_4)}{N(w_3)}$$\n",
    "\n",
    "Puede haber muchas probabilidades iguales a 0 $\\Rightarrow$ suavizado: Backoff, Kneser-Ney"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "5) Reconocimiento: Viterbi en la red completa: __Palabras (modelo de lenguaje) $\\Rightarrow$ HMM de Fonemas concatenados para cada palabra__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mezcla de gaussianas\n",
    "\n",
    "$$b_j(y)=\\sum_{m=1}^M c_{jm}\\mathcal{N}(y;\\mu_{jm},\\Sigma_{jm})$$\n",
    "\n",
    "<img src=\"mixture.png\" width=600 height=600 style=\"display:block; margin-left: auto; margin-right: auto;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Baum-Welch\n",
    "\n",
    "$$\\gamma_t(j,m)=\\Bigg[\\frac{\\alpha_t(j)\\beta_t(j)}{\\sum_{i=1}^N\\alpha_t(i)\\beta_t(i)}\\Bigg]\\Bigg[{\\frac{c_{jm}b_{jm}(y_t)}{\\sum_{n=1}^M c_{jn}b_{jn}(y_t)}}\\Bigg]$$\n",
    "\n",
    "$$\\mu_{jm}=\\frac{\\sum_{t=1}^T\\gamma_t(j,m)y_t}{\\sum_{t=1}^T\\gamma_t(j,m)}$$\n",
    "\n",
    "$$c_{jm}=\\frac{\\sum_{t=1}^T\\gamma_t(j,m)}{\\sum_{t=1}^T\\sum_{n=1}^M\\gamma_t(j,n)}$$\n",
    "\n",
    "\n",
    "<center>Lo mismo para $\\Sigma_{jm}$</center>\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
